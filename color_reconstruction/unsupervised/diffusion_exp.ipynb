{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diffusion experiments",
   "id": "ccecde4b2c96cff4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "50e1c336a493b788"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:04:57.167233Z",
     "start_time": "2024-11-09T23:04:55.903440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision import datasets\n",
    "import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from unet import Unet\n",
    "from denoiser import Diffusion"
   ],
   "id": "1f05c1efa411b45e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### plot utils",
   "id": "34faf54df6685d55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T23:04:57.173344Z",
     "start_time": "2024-11-09T23:04:57.170442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def show_images(original, noisy, reconstructed):\n",
    "    original = np.transpose(original.cpu().numpy(), (1, 2, 0))\n",
    "    noisy = np.transpose(noisy.cpu().numpy(), (1, 2, 0))\n",
    "    reconstructed = np.transpose(reconstructed.detach().cpu().numpy(), (1, 2, 0))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title(\"Image Originale\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(noisy)\n",
    "    axes[1].set_title(\"Image Bruitée\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(reconstructed)\n",
    "    axes[2].set_title(\"Image Reconstituée\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ],
   "id": "88f7da3221f73223",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment CelebA",
   "id": "e3420df16241dc9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up data loader\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "transforms.Lambda(lambda x: 2 * x - 1),\n",
    "transforms.Resize((64,64))])\n",
    "#train_dataset = datasets.CIFAR10(root=\"data\", train=True, transform=transform, download=True)\n",
    "train_dataset = datasets.CelebA(root=\"data\", transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "def gene():\n",
    "    for elt in tqdm.tqdm(train_loader):\n",
    "        yield elt[0]\n",
    "# Initialize diffusion process and model\n",
    "timesteps = 1000\n",
    "t_reconst=200\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\")\n",
    "model = Unet(channels=64).to(device)\n",
    "nb_params = sum([elt.numel() for elt in model.parameters()])\n",
    "print(f\"nb params : {nb_params}\")\n",
    "diffusion = Diffusion(model, timesteps=timesteps, device=device)\n",
    "\n",
    "full_image = next(gene())[0].unsqueeze(0)  # Charger une image\n",
    "# Train model\n",
    "diffusion.train(gene, num_epochs=num_epochs)\n",
    "shape = full_image.shape\n",
    "img_gen = diffusion.sampling(shape)\n",
    "plt.imshow(np.clip(img_gen[0].permute(2,1,0).detach().cpu().numpy(), 0, 1))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "noisy_img = full_image.clone()\n",
    "# noisy_img[0, :, :] = 0\n",
    "noisy_img, _ = diffusion.forward_diffusion(noisy_img, t_reconst)\n",
    "noisy_img = noisy_img.squeeze(0)\n",
    "reconstructed_full_image = diffusion.sampling(shape=None, xT=noisy_img, T=t_reconst).squeeze(0)\n",
    "# reconstructed_full_image = diffusion.sampling(shape = noisy_img.shape, T=t_reconst).squeeze(0).clip(0., 1.)\n",
    "# reconstructed_full_image = diffusion.sampling(xT=reconstructed_full_image, T=20).squeeze(0).clip(0., 1.)\n",
    "# Afficher les résultats\n",
    "\n",
    "\n",
    "show_images((1 + full_image[0]) / 2, (1 + noisy_img) / 2, (1 + reconstructed_full_image) / 2)\n",
    "\n",
    "torch.save(model.state_dict(), \"model_latest.pt\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Experiment HE",
   "id": "b6fa3c9c5a26cfd8"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-09T23:21:53.151550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "# Chargement du dataset avec extraction de patchs\n",
    "class HistoLiverPatchDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, nb_steps=1000, patch_size=64, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.images = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if\n",
    "                       img.endswith('.jpg') or img.endswith('.png')]\n",
    "\n",
    "        self.np_images = [Image.open(img_path).convert(\"RGB\") for img_path in self.images]\n",
    "        self.nb_img = len(self.images)\n",
    "        self.nb_steps = nb_steps\n",
    "    def __len__(self):\n",
    "        return self.nb_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly select an image from the list\n",
    "        image_idx = np.random.choice(range(self.nb_img))\n",
    "        image = self.np_images[image_idx]\n",
    "        # Convert to tensor temporarily to get width and height\n",
    "        image_tensor = 2*transforms.ToTensor()(image)-1\n",
    "        _, img_height, img_width = image_tensor.shape\n",
    "\n",
    "        # Randomly select the top-left corner of the patch\n",
    "        max_row = img_height - self.patch_size\n",
    "        max_col = img_width - self.patch_size\n",
    "        row = np.random.randint(0, max_row)\n",
    "        col = np.random.randint(0, max_col)\n",
    "\n",
    "        # Crop the patch from the image\n",
    "        patch = image_tensor[:, row:row + self.patch_size, col:col + self.patch_size]\n",
    "\n",
    "        return patch\n",
    "\n",
    "# Définir les transformations pour les images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Charger le dataset de patchs\n",
    "image_dir = '../../dataset/data/liver_HE'  # Assurez-vous que ce dossier contient vos images\n",
    "patch_size = 64\n",
    "batch_size = 64\n",
    "nb_patch_per_epoch = 6400\n",
    "dataset = HistoLiverPatchDataset(nb_steps= nb_patch_per_epoch ,image_dir=image_dir, patch_size=patch_size, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "timesteps = 1000\n",
    "t_reconst = 100\n",
    "device = torch.device(\"cuda\")\n",
    "model = Unet(channels=100).to(device)\n",
    "nb_params = sum([elt.numel() for elt in model.parameters()])\n",
    "print(device)\n",
    "print(f\"nb params : {nb_params}\")\n",
    "diffusion = Diffusion(model, timesteps=timesteps, beta_end=2e-2,device=device)\n",
    "diffusion.train(dataloader, lr=1e-3, num_epochs=100)\n",
    "\n",
    "# Chargement d'une image complète et affichage des résultats\n",
    "full_image = dataset[0][:,:patch_size,:patch_size]  # Charger une image\n",
    "noisy_img = full_image.clone()\n",
    "#noisy_img[0, :, :] = 0\n",
    "noisy_img, _ = diffusion.forward_diffusion(noisy_img, t_reconst)\n",
    "noisy_img = noisy_img.squeeze(0)\n",
    "reconstructed_full_image = diffusion.sampling(shape=None,xT=noisy_img, T=t_reconst).squeeze(0)\n",
    "#reconstructed_full_image = diffusion.sampling(shape = noisy_img.shape, T=t_reconst).squeeze(0).clip(0., 1.)\n",
    "#reconstructed_full_image = diffusion.sampling(xT=reconstructed_full_image, T=20).squeeze(0).clip(0., 1.)\n",
    "# Afficher les résultats\n",
    "\n",
    "\n",
    "show_images((1+full_image)/2, (1+noisy_img)/2, (1+reconstructed_full_image)/2)"
   ],
   "id": "d9041136da6a8582",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "nb params : 11434115\n",
      "Epoch 1/100, Loss: 0.2838, nb_step: 100\n",
      "Epoch 2/100, Loss: 0.1674, nb_step: 100\n",
      "Epoch 3/100, Loss: 0.1242, nb_step: 100\n",
      "Epoch 4/100, Loss: 0.08726, nb_step: 100\n",
      "Epoch 5/100, Loss: 0.0986, nb_step: 100\n",
      "Epoch 6/100, Loss: 0.09496, nb_step: 100\n",
      "Epoch 7/100, Loss: 0.07203, nb_step: 100\n",
      "Epoch 8/100, Loss: 0.06779, nb_step: 100\n",
      "Epoch 9/100, Loss: 0.06251, nb_step: 100\n",
      "Epoch 10/100, Loss: 0.07291, nb_step: 100\n",
      "Epoch 11/100, Loss: 0.07452, nb_step: 100\n",
      "Epoch 12/100, Loss: 0.09058, nb_step: 100\n",
      "Epoch 13/100, Loss: 0.05127, nb_step: 100\n",
      "Epoch 14/100, Loss: 0.06355, nb_step: 100\n",
      "Epoch 15/100, Loss: 0.08679, nb_step: 100\n",
      "Epoch 16/100, Loss: 0.06474, nb_step: 100\n",
      "Epoch 17/100, Loss: 0.05874, nb_step: 100\n",
      "Epoch 18/100, Loss: 0.05672, nb_step: 100\n",
      "Epoch 19/100, Loss: 0.06628, nb_step: 100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "539b32e49b2163f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
